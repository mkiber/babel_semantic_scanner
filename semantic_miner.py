import json
import faiss
import random
import numpy as np
from pathlib import Path
from sentence_transformers import SentenceTransformer
from cpp_generator import cpp_generate_page  # –ø–æ–¥–∫–ª—é—á–∞–µ–º C++ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä

# === –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è ===
MODEL_NAME = "deepvk/USER-base"
INDEX_PATH = "anchor.index"
VECTORS_PATH = "anchor_vectors.json"
RESULTS_PATH = Path("semantic_hits.jsonl")

HEX_SPACE = "abcdefghijklmnopqrstuvwxyz0123456789"
MAX_PAGES = 100
SIMILARITY_THRESHOLD = 0.90

# === –ó–∞–≥—Ä—É–∑–∫–∞ ===
model = SentenceTransformer(MODEL_NAME)
with open(VECTORS_PATH, "r", encoding="utf-8") as f:
    anchors = json.load(f)

index = faiss.read_index(INDEX_PATH)

def generate_random_hex_id(length=6):
    return ''.join(random.choices(HEX_SPACE, k=length))

def normalize(v):
    return v / np.linalg.norm(v)

def search_similar(text: str, top_k=3):
    emb = model.encode(text)
    emb = normalize(emb).astype("float32").reshape(1, -1)
    distances, indices = index.search(emb, top_k)

    results = []
    for idx, dist in zip(indices[0], distances[0]):
        similarity = 1 - dist
        if similarity >= SIMILARITY_THRESHOLD:
            results.append({
                "similarity": round(similarity, 4),
                "fragment": anchors[idx]["text"]
            })
    return results

# === –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª ===
hits = []
for _ in range(MAX_PAGES):
    hex_id = generate_random_hex_id()
    page_num = random.randint(0, 1000)

    try:
        text = cpp_generate_page(hex_id, page_num)
        matches = search_similar(text, top_k=3)

        if matches:
            print(f"üìå –ù–∞–π–¥–µ–Ω—ã —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –¥–ª—è {hex_id}:{page_num}")
            hits.append({
                "hex_id": hex_id,
                "page_num": page_num,
                "matches": matches,
                "text": text[:500]  # –ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤
            })
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞ {hex_id}:{page_num}: {e}")

# === –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ ===
if hits:
    with open(RESULTS_PATH, "w", encoding="utf-8") as f:
        for hit in hits:
            f.write(json.dumps(hit, ensure_ascii=False) + "\n")

    print(f"\n‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {len(hits)} —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π ‚Üí {RESULTS_PATH}")
else:
    print("üòï –°–æ–≤–ø–∞–¥–µ–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
