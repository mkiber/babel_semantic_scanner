import os
import json
import faiss
import numpy as np
from generator import RussianBabelGenerator
from sentence_transformers import SentenceTransformer
from tqdm import tqdm
from uuid import uuid4

# === –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ===
MODEL_NAME = "deepvk/USER-base"
INDEX_PATH = "anchor.index"
VECTORS_PATH = "anchor_vectors.json"
OUTPUT_PATH = "semantic_matches.jsonl"
LOG_PATH = "semantic_matches.log"
BOOKS_TO_SCAN = 10
PAGES_PER_BOOK = 10
SIMILARITY_THRESHOLD = 0.90

# === –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ –∏–Ω–¥–µ–∫—Å–∞ ===
model = SentenceTransformer(MODEL_NAME)
with open(VECTORS_PATH, "r", encoding="utf-8") as f:
    anchor_data = json.load(f)
index = faiss.read_index(INDEX_PATH)

# === –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ ===
generator = RussianBabelGenerator()

# === –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è ===
def search_across_books():
    matches = []
    seen_hexes = set()

    with open(LOG_PATH, "w", encoding="utf-8") as log:
        for _ in tqdm(range(BOOKS_TO_SCAN), desc="üìö –ö–Ω–∏–≥–∏"):
            # –£–Ω–∏–∫–∞–ª—å–Ω—ã–π hex_id
            while True:
                hex_id = uuid4().hex
                if hex_id not in seen_hexes:
                    seen_hexes.add(hex_id)
                    break

            for page_num in range(PAGES_PER_BOOK):
                text = generator.generate_page(hex_id, page_num)
                embedding = model.encode(text)
                embedding = embedding / np.linalg.norm(embedding)
                embedding = np.array([embedding]).astype("float32")

                distances, indices = index.search(embedding, 3)

                for idx, dist in zip(indices[0], distances[0]):
                    similarity = 1 - dist
                    if similarity >= SIMILARITY_THRESHOLD:
                        match = {
                            "hex_id": hex_id,
                            "page_num": page_num,
                            "similarity": round(similarity, 4),
                            "anchor_fragment": anchor_data[idx]["text"].strip(),
                            "page_text": text[:300] + ("..." if len(text) > 300 else "")
                        }
                        matches.append(match)
                        print(f"\nüîé –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ!")
                        print(f"üìò –°—Ç—Ä–∞–Ω–∏—Ü–∞: hex_id={hex_id}, page_num={page_num}")
                        print(f"üß† –°—Ö–æ–¥—Å—Ç–≤–æ: {similarity:.4f}")
                        print(f"üìù –§—Ä–∞–≥–º–µ–Ω—Ç:\n{text[:300]}\n")
                        log.write(f"MATCH {hex_id}:{page_num} ‚Üí similarity {similarity:.4f}\n")

                        break  # –û–¥–∏–Ω –º–∞—Ç—á –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ

    if matches:
        with open(OUTPUT_PATH, "w", encoding="utf-8") as f:
            for match in matches:
                f.write(json.dumps(match, ensure_ascii=False) + "\n")
        print(f"\n‚úÖ –ù–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {len(matches)}. –†–µ–∑—É–ª—å—Ç–∞—Ç ‚Üí {OUTPUT_PATH}")
    else:
        print("üòï –°–æ–≤–ø–∞–¥–µ–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")

if __name__ == "__main__":
    search_across_books()
